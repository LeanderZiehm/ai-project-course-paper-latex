\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{enumitem}

\title{LLM Anonymization Techniques Recommender for Datasets (LLMANO-2)}
\author{Amin Akziz, Leander Ziehm, Azba Engineer, Santiago}
\date{April 2025}

\begin{document}

\maketitle

\section{Introduction}
The need for anonymizing datasets has grown significantly with the proliferation of machine learning and large language models (LLMs). However, most users lack a clear understanding of how to properly anonymize their data without significantly reducing its utility. Our work introduces a recommender system that leverages LLMs to guide users in anonymizing CSV datasets while preserving predictive performance. As motivation, we reference recent data privacy incidents that highlight the risks of improper anonymization.

\section{Background/Theory}
We outline key privacy concepts including:
\begin{itemize}
    \item \textbf{Direct identifiers} (e.g., names, emails)
    \item \textbf{Quasi-identifiers} (e.g., weight, height, age)
    \item \textbf{Anonymity metrics} such as $k$-Anonymity and $l$-Diversity
\end{itemize}
We also introduce user-driven anonymization processes and the limitations of manual or rule-based methods.

In this section, we outline the mathematical foundations underlying key privacy-preserving techniques in data publishing, namely $k$-anonymity, $l$-diversity, and generalization strategies.

\subsection{Identifiers and Quasi-Identifiers}

In the context of privacy-preserving data analysis, it is essential to distinguish between different types of attributes:

\begin{itemize}
\item \textbf{Direct Identifiers} are attributes that can uniquely identify an individual without external information. Common examples include names, social security numbers, or email addresses.

\item \textbf{Indirect Identifiers}, also called \textit{quasi-identifiers}, are attributes that do not uniquely identify an individual on their own but can do so when combined with other quasi-identifiers. Examples include age, ZIP code, and gender.
\end{itemize}

Effective anonymization techniques aim to remove or transform these identifiers such that the risk of re-identification is minimized.

\subsection{$k$-Anonymity}

The concept of $k$-anonymity, introduced by Sweeney, is one of the foundational principles for privacy protection. A dataset is said to satisfy \textbf{$k$-anonymity} if each record is indistinguishable from at least $k-1$ other records with respect to the quasi-identifiers.

Formally, given a dataset $D$ with quasi-identifier attributes $Q_1, Q_2, ..., Q_m$, let $\pi$ be a projection onto the quasi-identifier space:

$$
\pi(D) = \{ (q_1, q_2, ..., q_m) \mid (q_1, ..., q_m) \in D \}
$$

Let $E$ be an equivalence class of records in $D$ that share the same quasi-identifier values:

$$
E = \{ r \in D \mid \forall i, \; r[Q_i] = v_i \}
$$

Then $D$ satisfies $k$-anonymity if:

$$
\forall E \subseteq D, \quad |E| \geq k
$$

This ensures that an adversary cannot link any given record to fewer than $k$ individuals. However, $k$-anonymity alone does not prevent \textit{attribute disclosure}â€”when sensitive values within a group are too homogeneous.

\subsection{$l$-Diversity}

To address the limitations of $k$-anonymity, the notion of \textbf{$l$-diversity} was proposed. It enhances privacy by requiring that each equivalence class (i.e., group of records sharing the same quasi-identifiers) contains at least $l$ "well-represented" values for the sensitive attribute.

Mathematically, let $S$ be the sensitive attribute. An equivalence class $E$ satisfies \textit{distinct $l$-diversity} if:

$$
|\{ r[S] \mid r \in E \}| \geq l
$$

This means that in each equivalence class, there are at least $l$ distinct values for the sensitive attribute.

More robust variants include:
\begin{itemize}
\item \textbf{Entropy $l$-Diversity}, which requires the entropy of the sensitive attribute distribution in each class to be at least $\log l$:
$$
-\sum_{s \in S} p_s \log p_s \geq \log l
$$
where $p_s$ is the fraction of records in the class with sensitive value $s$.

\item \textbf{Recursive (c, $l$)-Diversity}, which prevents skewness by limiting how dominant the most frequent sensitive value can be.
\end{itemize}

\subsection{Generalization Strategies}

Achieving $k$-anonymity and $l$-diversity typically requires generalization and/or suppression of quasi-identifier attributes. Generalization replaces specific values with broader categories. For example:
\begin{itemize}
\item Age: 23 $\rightarrow$ [21--30]
\item ZIP Code: 12345 $\rightarrow$ 123**
\end{itemize}

The goal is to form equivalence classes with sufficient cardinality and sensitive attribute diversity, while minimizing information loss.


\section{Architecture}
\begin{itemize}
    \item Setup and hardware needed
    \item Overview diagram of system components
    \item LLM interface and decision flow
    \item Form interface for user inputs (direct/quasi identifier tagging)
    \item Backend anonymization and evaluation pipeline
\end{itemize}

\section{Related Work}
This section reviews:
\begin{itemize}
     \item The paper where we got the from from
    \item LLM-based anonymization
    \item Rule-based anonymization tools
    \item Human-led anonymization efforts
\end{itemize}
We discuss the differences in our approach, emphasizing interactive recommendation and fine-grained generalization control.

\section{Methodology}
Our system allows users to:
\begin{enumerate}
    \item Upload CSV datasets
    \item Select columns as direct or quasi-identifiers (form includes definitions and examples)
    \item Receive LLM-generated anonymization recommendations (e.g., $k=10$)
    \item Optionally adjust granularity and generalization steps
\end{enumerate}
LLMs are prompted to recommend anonymization strategies based on dataset structure and user inputs. Screenshots and/or GitHub links will be included.


\section{Results}
We test our system on multiple datasets:
\begin{itemize}
    \item Comparison against human-created anonymization
    \item Benchmarking against other anonymization algorithms
    \item Metrics: preservation of predictive accuracy, anonymity scores
\end{itemize}
Each configuration is tested 10 times to compute average performance. Evaluation is automated and requires no user interaction.

\section{Limitations, Discussion, and Future Work}
\begin{itemize}
    \item Current setup lacks real-time chat interaction for refinement
    \item Scope limited to $k$-Anonymity and $l$-Diversity; future integration of differential privacy
    \item Broader comparison with more anonymization techniques needed
    \item Challenges in dataset diversity and granularity recommendation
\end{itemize}

\section{Conclusion}
We presented a prototype LLM-powered anonymization recommender that assists users in balancing privacy with data utility. Our system enables informed anonymization decisions and shows promise compared to existing approaches. Future enhancements will include conversational interfaces and broader evaluation.

\end{document}
